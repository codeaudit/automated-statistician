%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most modern people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
% \usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
% \usepackage{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2015}

% Custom libraries.
\usepackage{amsmath}
\usepackage{amssymb}

% Custom commands.
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathbb{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\N}[1]{\mathcal{N}\left(#1\right)}
\newcommand{\paren}[1]{\left({#1}\right)}
\newcommand{\brac}[1]{\left[{#1}\right]}
\renewcommand{\Pr}[1]{\mathbb{P}\brac{{#1}}}
\newcommand{\set}[1]{\left\{{#1}\right\}}
\newcommand{\T}{\boldsymbol{\Theta}}
\newcommand{\abs}[1]{\left|{#1}\right|}
\newcommand{\norm}[1]{\abs{\abs{{#1}}}_2^2}
\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}

\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\title{Automated Statistician}
\icmltitlerunning{Automated Model Selection via Gaussian Processes}

\begin{document} 

\twocolumn[
\icmltitle{Automated Model Selection via Gaussian Processes}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Rui Shu}{rshu15@stanford.edu}
\icmlauthor{Stephen Koo}{sckoo@stanford.edu}
\icmladdress{AA 228, Stanford University}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{bayesian optimization, acquisition functions}

\vskip 0.3in
]

\begin{abstract} 
Lorem ipsum.
\end{abstract} 

\section{Introduction}
Talk about the difficulty of model selection. There are a lot of techniques out there for variable selection. This doesn't address the question of selecting the model family. There has been recent interest in the use of deep neural networks, but these often require large amounts of data to be trained properly, are computationally costly, are difficult to interpret and intuit, and require careful tuning of hyperparameters to train successfully. There are also methods for hyperparameter tuning, but we still need to decide which family of model! Ultimately, these approaches simply abstracts human-assisted model selection to a higher level: that of determining which of these approaches to even use in the first place. 

There are general rules of thumb. Always start with a simple model and gradually work our way to harder models. The question then becomes whether we can automate this process of model selection in its entirety. We propose to do so by formulating the question as a POMDP that trades-off between model complexity and model accuracy.

\section{Hyperparameter Selection}
Describe the formulation in a single-model setting, where the objective is simply to find the best hyper using Gaussian process. Formalize this with math about test set performance. 

\subsection{Gaussian Processes}
The task of hyperparameter selection has often been formulated as a function optimization procedure performed that involves Gaussian process regression. Gaussian process regression is a non-parametric Bayesian modeling tool that imposes a prior distribution over $f$ and updates its belief about the distribution upon the observation of data. Formally, a Gaussian process is parameterized by a mean function $m(\x)$ and a kernel $k(\x, \x')$, which we define as,
\begin{align}
  m(\x) &= \E{f(\x)} \\
  k(\x, \x') &= \E{(f(\x) - m(\x))(f(\x') - m(\x'))},
\end{align}
which can be rewritten as $f(\x) \sim \mathcal{GP}(m(\x), k(\x, \x'))$. The exact choice of the functions $m$ and $k$ are priors required by the Gaussian process and the proper selection of such priors is describes in the next section. Crucially, a Gaussian process simply describes a collection of random variables $\set{f(\x_i)}_{1:n}$ drawn from a multivariate Gaussian distribution, and can thus be rewritten as,
\begin{align}
  \begin{bmatrix}
    f_1 \\ \vdots \\ f_n
  \end{bmatrix} \sim
  \N{\begin{bmatrix} m_1 \\ \vdots \\ m_n \end{bmatrix},
    \begin{bmatrix}
      k_{11} & \ldots & k_{1n} \\
      \vdots & \ddots & \vdots \\
      k_{n1} & \ldots & k_{nn}
  \end{bmatrix}},
\end{align}
where the dependency on $\x_{1:n}$ ($f_i = f(\x_i)$, $m_i = m(\x_i)$ and $k_{ij} = k(\x_i, \x_j)$) is suppressed for notational simplicity. For further simplicity, we write,
\begin{align}
  f_{1:n} \sim \N{m_{1:n}, k_{(1:n) \times (1:n)}}.
\end{align}

When seen as a multivariable Gaussian distributtion, it is easy to see how updating the Gaussian process involves computing a conditional Gaussian distribution. Supposing that the first $n-1$ points $f_{1:n-1}$ were observed, we compute the conditional distribution as,
\begin{align}
  \begin{bmatrix}
    f_{1:n-1} \\ f_n
  \end{bmatrix} &\sim
  \N{
    \begin{bmatrix} m_{1:n-1} \\  m_n \end{bmatrix},
    \begin{bmatrix}
      k_\alpha  & k_\beta \\
      k_\beta^\top  & k_\gamma
    \end{bmatrix}
  }, \\
  \begin{split}
    f_n \mid f_{1:n-1} &\sim
  \mathcal{N} (m_\D(x_n), k_\D(x_n, x_n)),
  \end{split}\\
  m_\D &= m_n + k_\beta^\top k_\alpha^{-1}(f_{1:n-1} - m_{1:n-1}), \\
  k_\D &= k_\gamma - k_\beta^\top k_\alpha^{-1} k_\beta,
\end{align}
where $k_\alpha = k_{(1:n-1) \times (1:n-1)}$, $k_\beta = k_{(1:n-1) \times n}$, and where $k_\gamma = k_{nn}$. Here, $m_\D$ and $k_\D$ denote the posterior mean and covariance functions upon observation of the data $\D = f_{1:n-1}$. Since it is often the case that realizations of the random variables $f_{1:n-1}$ are observed with some $\sigma$-Gaussian noise, it is common to use $k_\alpha = k_{(1:n-1) \times (1:n-1)} + \sigma^2I_n$. Crucially, performing Gaussian process regression returns not only a posterior mean function, but also a posterior covariance function that properly captures the uncertainty of the interpolation. It is this ability of a Bayesian modeling tool to reason about the uncertainty of the system makes Gaussian processes for the online reinforcement learning task of function optimization.

\subsubsection{Selection of Mean}
Despite being non-parameteric, a Gaussian process is not completely free-form and does require the imposition of certain priors, namely the mean and kernel functions. Since it is often the case that the regression task is performed within the unit hypercube, a zero-mean Gaussian process where $m(\x) = 0$ will often suffice and its impact on the posterior mean diminishes quickly with the introduction of more data. It is sometimes useful, however, to choose a quadratic prior.

\subsubsection{Selection of Kernel}
Perhaps of greater importance to the behavior of the Gaussian process is the kernel function. The choice of kernel functions can greatly impact the posterior mean and covariance of the Gaussian process. A plethora of kernel functions exist, ranging from periodic kernels to the Mat\'{e}rn and squared exponential kernel. Extensive research has been done on the performance of composite kernels generated from the linear combination of a set of basis kernels. Within the context of test-set performance function, however, if one is reasonably confident that the performance function varies smoothly over the hyperparameter space, it is often enough to impose the squared exponential kernel,
\begin{align}
  k(\x, \x') = \theta_0^2\exp\paren{-\frac{1}{2} \gamma\paren{\x, \x'}},
\end{align}
where $\gamma\paren{\x,\y}=\sum_{i=1}^k \frac{\paren{x_i-x'_i}^2}{\theta_i^2}$. It is still necessary to learn the parameters $\theta_{0:k}$. Of especial importance are the length scale parameters $\theta_{1:k}$, which reflect the extend to which observed points can influence the interpolation in its neighborhood. A large length scale reflects a greater sphere of influence, and vice versa. To learn these parameters, we choose the parameters $\theta_{0:k}$ that maximize the log marginal likelihood,
\begin{align}
  \begin{split}
    &\log p \left(f_{1:n} \vert \x_{1:n}, \theta_{0:k}\right) \propto \\
    &-(f_{1:n} - m_{1:n})^\top k_\alpha^{-1}(f_{1:n} - m_{1:n}) - \log \det k_\alpha,
  \end{split}
\end{align}
which can be performed via any gradient ascent algorithm. The term involving $\log \det k_\alpha$ can be interpreted as a regularization parameter that encourages longer length scales (i.e. a low variance model) as long as such simplicity does not greatly hinder the model's ability to fit the data, which is captured by the quadratic form $f^\top k_\alpha^{-1}f$. Since $k_\alpha$ is also a function of the noise parameter $\sigma$, it is also possible to learn the value of $\sigma$ via gradient ascent as well. However, introducing $\sigma$ as a learnable paramater often produces many local optima that reflects a different interpretation of the data; a large $\sigma$ is often accompanied by the learning of larger length scales, reflecting a low-variance high-bias interpolation, whereas a small $\sigma$ is accompanied by small length scales, reflecting a high-variance low-bias interpolation. Within the context of hyperparameter selection, the noise parameter is chosen beforehand and noted in the Experimental section.


\subsection{Sequential Hyperparameter Selection}
The use of a Gaussian process provides the Bayesian approach to modeling the uncertainty of the system, namely the confidence of our interpolated performance function. In an online function optimization framework, however, a sequential series of hyperparamters must be chosen, with the hope that each choice in the series allows us to make a more informed decision about the identity of the hyperparameters. To do so, it is important for the online system to properly manage exploration and exploitation. It is possible to formulate this task as a Belief-State Markov Decision Process where the belief-state is our belief about the latent performance function as capture by the Guassian process, the action-space is the set of all possible hyperparameters to choose from, and the observation-space is the set of all possible performance function evaluations at a particular hyperparameter. However, the continuous nature of the action and observation space makes most online policies intractable without discretization. It is thus common within the hyperparameter optimization literature to adopt the use of an acquisition function\textemdash a deterministic function of the posterior mean and covariance from the Gaussian process that allows for a simple albeit ad-hoc trade-off between exploration and exploitation. 

As is the case with kernel selection, many acquisition functions exist. In our experiments, we use the upper-confidence bound for hyperparameter selection, defined as,
\begin{align}
  m_D(\x) + 2\sqrt{k_D(\x, \x)}.
\end{align}
The key benefit of such an acquisition function is that it is easily differentiable with respect to $\x$, enabling the use of gradient ascent to identify where the acquisition function is maximized without the need of evaluating the Gaussian process on a pre-defined grid of hyperparameters. 

\section{Model Selection}
While hyperparameter optimization has been extensively studied, there has been few research done on the hyperparameter optimization in a multi-model setting. Research in model selection has largely been focus on variable selection and hyperparameter tuning constrained within a single family of model, such as least-squares regression. As the number of such approaches increases, it is natural to consider the possibility of extending the task of Bayesian optimization to include not only hyperparameter selection, but also model family selection. Here, we describe our formulation of the problem as an multi-armed Gaussian bandit. 

\subsection{Multi-Armed Gaussian Bandit}
A multi-armed Gaussian Bandit problem addresses the question of optimal decision making 

\subsection{Sequential Model Selection}

\section{Experiments}
\subsection{Experimental Details}
We test our approach v. random sampling in a limit time duration.

\subsection{Results}

\section{Discussion}
Interesting stuff regarding how to leverage information across datasets. Limitations regarding acquisition function optimization. 

Because action and state space are both continuous. 

% Figures I want:
% First Figure:
% 1. A simple figure showing Gaussian process and the next point selection
% 2. The best selected hyperparameters as a function of time. 
% Second Figure:
% 1. 

% TODO: 
% Add the following citations to main.bib
% Gaussian Process book (Rasmussen)
% Cite Random Search from Bengio
% 
\bibliography{main}
\bibliographystyle{icml2015}
\end{document} 


